"Філософсько-етичні проблеми у сфері штучного інтелекту"
Питання життя та смерті {-=>—:::error_type=Punctuation} одвічне філософське питання, яке більшість людей {вирішую=>вирішує:::error_type=Spelling} {по своєму=>по-своєму:::error_type=Spelling}. А що{,=>:::error_type=Punctuation} {якщо=>коли:::error_type=Fluency} { =>:::error_type=Punctuation}я скажу{=>,:::error_type=Punctuation} що це питання потребує негайного {і=>й:::error_type=Spelling} одноголосного рішення? 
За останні {декілька=>кілька:::error_type=Fluency} років прогрес у сфері штучного інтелекту та машинного навчання зробив величезний крок {вперед=>уперед:::error_type=Spelling}. Більшість людей вже {звикли=>звикла:::error_type=Grammar} до смартових речей у нашому житті — {фітнес трекерів=>фітнес-трекерів:::error_type=Spelling} з вимірюванням пульсу, кардіограми та багатьох біометричних показників, смартфонів з розблокуванням сітківкою чи власним обличчям, для багатьох навіть роботи доставки {=>":::error_type=Punctuation}Амазон{=>":::error_type=Punctuation} на вулицях {Силіконової=>Кремнієвої:::error_type=Fluency} долини {=>— :::error_type=Punctuation}звичайна річ. Насправді процес сягає великих проривів {в=>у:::error_type=Spelling} медицині. Зараз системи розпізнавання зображень здатні визначити наявність певних видів раку чи наявність пневмонії {з якістю кращою=>краще:::error_type=Fluency} {ніж=>за:::error_type=Fluency} {кваліфіковані лікарі=>кваліфікованих лікарів:::error_type=Grammar}. Більшість {з=>із:::error_type=Spelling} нас не {задумуються=>задумується:::error_type=Grammar} про етичні та філософські проблеми{=>,:::error_type=Punctuation} з якими стикаються спеціалісти з штучного інтелекту, які розробляють технологічні рішення.
У серпні цього року науковий співробітник {з=>із:::error_type=Spelling} комп’ютерного зору та робототехніки {університету Кембрідж=>Кембриджського університету:::error_type=Fluency} Алекс Кендал опублікував роздум ”Let`s talk about Artificial intelligence”, де {він=>:::error_type=Fluency} {розглядав=>розглянув:::error_type=Grammar} конкретні етичні проблеми спеціалістів {з=>із:::error_type=Spelling} машинного навчання. Три {ключових=>ключові:::error_type=Grammar} проблеми, які він розглянув, це: довіра, чесність та справедливість.
Якщо чесність вимагає в основному легкого інтерпретування рішень, тобто вміння алгоритмом надати певні суттєві причини для деякого рішення, то з довірою та справедливістю все набагато складніше. 
Справедливість. Розглянемо доволі простий приклад. Згідно з TrafficSTATS study by CMU for AAA (2007) чоловіки мають на 77% {вищій=>вищий:::error_type=Spelling} ризик померти в автомобільній аварії, ніж жінка. Тобто{,=>:::error_type=Punctuation} якщо подібні {данні=>дані:::error_type=Spelling} включають {в=>у:::error_type=Spelling} модель{=>,:::error_type=Punctuation} яка визначає {імовірність=>ймовірність:::error_type=Spelling} потрапити в аварію та вартість страхового полісу, вона матиме кращу точність, тобто буде справедливішою до реальної статистики. Проте, згідно з рішенням {Европейського=>Європейського:::error_type=Spelling} суду справедливості у 2011 році, стать не може бути врахована у вартість страхового полісу як фактор ризику, бо {вважається=>вважатиметься:::error_type=Grammar} {дескримінацією=>дискримінацією:::error_type=Spelling}.
Іншими виявленими прикладами нечесних / необ’єктивних систем є: система розпізнавання голосу компанії Google систематично краще справляється з чоловічим голосом{=>,:::error_type=Punctuation} ніж {з=>із:::error_type=Spelling} жіночим, система підрахунку ризику повторного правопорушення необ’єктивна щодо {афро-американців=>афроамериканців:::error_type=Spelling}. 
{Нажаль=>На жаль:::error_type=Spelling}, системи штучного інтелекту мають властивість наслідувати упередженість від людей. Вирішення цієї проблеми полягає в використанні {більш якісних=>якісніших:::error_type=Fluency} датасетів для кращого вивчення маленьких (рідкісних) класів, збільшенні якості методів збору та обробки даних, для зменшення упередженості даних.
Якщо зі справедливістю кроки доволі технічні, то з довірою все набагато складніше. Критично важливим є те, що користувачі довіряють системам штучного інтелекту. Якщо система не має належного рівня довіри, то ми навряд {будемо її використовувати=>її використовуватимемо:::error_type=Grammar}. Для побудови безпечних систем машинного навчання{,=>:::error_type=Punctuation} потрібно показати високу точність алгоритму, будувати алгоритми, які не тільки мають високу точність {вирішення задачі=>розв'язання завдання:::error_type=Fluency}, а й {вміють=>уміють:::error_type=Spelling} визначати неточність своїх передбачень {та=>і:::error_type=Spelling} розуміють, що є речі, {які =>:::error_type=Fluency}їм {не відомі=>невідомі:::error_type=Spelling} ({тобто,=>:::error_type=Fluency} наприклад, використовувати Баєсівські моделі глибокого навчання).
Повертаючись до проблеми життя та смерті. {у=>У:::error_type=Punctuation} 2015 році видання {=>":::error_type=Punctuation}MIT Technology {review=>Review:::error_type=Spelling}{=>":::error_type=Punctuation} опублікувало на тему “Why Self-Driving Cars Must Be Programmed to Kill”{.=>,:::error_type=Punctuation} {У тілі, якої=>у якій:::error_type=Fluency} представлено високо ймовірну ситуацію на дорозі, де хтось з великою ймовірністю загине{=>,:::error_type=Punctuation} лишається тільки обрати{=>,:::error_type=Punctuation} хто саме. Саме тому{,=>:::error_type=Punctuation} на {даний=>цей:::error_type=Fluency} момент ми не маємо повністю самокерованих машин. 
Більшість цих проблем зараз {лягають=>лягає:::error_type=Grammar} на плечі розробників, проте ми не можемо точно сказати{=>,:::error_type=Punctuation} чи {їх=>їхні:::error_type=Grammar} рішення будуть справедливими, оскільки люди все ще лишаються людьми. Очевидно тільки одне, на {даний=>цей:::error_type=Fluency} момент спеціалісти з штучного інтелекту можуть вплинути на тільки на технологічну сферу, а також на етику та філософію. Всі описані вище проблеми потребують найшвидшого вирішення, інакше штучний інтелект може не виправдати сподівань людей і настане наступна технологічна зима.


