{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MOFA+: training a model in Python\n",
    "Author: Ricard Argelaguet.    \n",
    "Affiliation: European Bioinformatics Institute, Cambridge, UK.  \n",
    "Date: 13/11/2019  \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook contains a detailed tutorial on how to train MOFA using Python.\n",
    "A template script to run the code below can be found [here](https://github.com/bioFAM/MOFA2/blob/master/template_script.py)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1) Load libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-06T20:25:16.898360Z",
     "start_time": "2020-02-06T20:25:16.836870Z"
    },
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "        #########################################################\n",
      "        ###           __  __  ____  ______                    ### \n",
      "        ###          |  \\/  |/ __ \\|  ____/\\    _             ### \n",
      "        ###          | \\  / | |  | | |__ /  \\ _| |_           ### \n",
      "        ###          | |\\/| | |  | |  __/ /\\ \\_   _|          ###\n",
      "        ###          | |  | | |__| | | / ____ \\|_|            ###\n",
      "        ###          |_|  |_|\\____/|_|/_/    \\_\\              ###\n",
      "        ###                                                   ### \n",
      "        ######################################################### \n",
      "       \n",
      " \n",
      "        \n"
     ]
    }
   ],
   "source": [
    "from mofapy2.run.entry_point import entry_point\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# initialise the entry point\n",
    "ent = entry_point()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2) Load data\n",
    "\n",
    "To create a MOFA+ object you need to specify four dimensions: samples (cells), features, view(s) and group(s). MOFA objects can be created from a wide range of input formats:\n",
    "\n",
    "### 2.1) pandas data.frame format\n",
    "A pandas data.frame with columns `sample`, `group`, `feature`, `view`, `value`.  This is the most intuitive format, as it summarises all omics/groups in a single data structure. Also, there is no need to add rows that correspond to missing data.\n",
    "\n",
    "For example:\n",
    "```\n",
    "sample  group   feature value   view\n",
    "sample1  groupA    gene1   2.8044  RNA\n",
    "sample1  groupA    gene3    2.2069  RNA\n",
    "sample2  groupB    gene2    0.1454  RNA\n",
    "sample2  groupB    gene1     2.7021  RNA\n",
    "sample2  groupB    promoter1    3.8618  Methylation\n",
    "sample3  groupB    promoter2    3.2545  Methylation\n",
    "sample3  groupB    promoter3   1.5014  Methylation\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we load a simulated data set with the following dimensions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-06T20:25:20.952029Z",
     "start_time": "2020-02-06T20:25:18.586476Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sample</th>\n",
       "      <th>group</th>\n",
       "      <th>feature</th>\n",
       "      <th>view</th>\n",
       "      <th>value</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>sample_0_group_0</td>\n",
       "      <td>group_0</td>\n",
       "      <td>feature_0_view_0</td>\n",
       "      <td>view_0</td>\n",
       "      <td>-2.05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>sample_1_group_0</td>\n",
       "      <td>group_0</td>\n",
       "      <td>feature_0_view_0</td>\n",
       "      <td>view_0</td>\n",
       "      <td>0.10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>sample_2_group_0</td>\n",
       "      <td>group_0</td>\n",
       "      <td>feature_0_view_0</td>\n",
       "      <td>view_0</td>\n",
       "      <td>1.44</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>sample_3_group_0</td>\n",
       "      <td>group_0</td>\n",
       "      <td>feature_0_view_0</td>\n",
       "      <td>view_0</td>\n",
       "      <td>-0.28</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>sample_4_group_0</td>\n",
       "      <td>group_0</td>\n",
       "      <td>feature_0_view_0</td>\n",
       "      <td>view_0</td>\n",
       "      <td>-0.88</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             sample    group           feature    view  value\n",
       "0  sample_0_group_0  group_0  feature_0_view_0  view_0  -2.05\n",
       "1  sample_1_group_0  group_0  feature_0_view_0  view_0   0.10\n",
       "2  sample_2_group_0  group_0  feature_0_view_0  view_0   1.44\n",
       "3  sample_3_group_0  group_0  feature_0_view_0  view_0  -0.28\n",
       "4  sample_4_group_0  group_0  feature_0_view_0  view_0  -0.88"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "D = [1000,1000] # Number of features per view\n",
    "M = len(D)      # Number of views\n",
    "K = 5           # Number of factors\n",
    "N = [100,100]   # Number of samples per group\n",
    "G = len(N)      # Number of groups\n",
    "\n",
    "data_dt = pd.read_csv(\"http://ftp.ebi.ac.uk/pub/databases/mofa/getting_started/data.txt.gz\", sep=\"\\t\")\n",
    "data_dt.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2) List of matrices\n",
    "A nested list of numpy arrays, where the first index refers to the view and the second index refers to the group. Samples are stored in the rows and features are stored in the columns. All views for a given group G must have the same samples in the rows. If there is any sample that is missing a particular view, the column needs to be filled with NAs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loading the same data above in matrix format:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-06T20:25:21.324421Z",
     "start_time": "2020-02-06T20:25:21.058184Z"
    },
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "data_prefix = \"http://ftp.ebi.ac.uk/pub/databases/mofa/getting_started\"\n",
    "data_mat = [[None for g in range(G)] for m in range(M)]\n",
    "for m in range(M):\n",
    "    for g in range(G):\n",
    "        data_mat[m][g] = np.loadtxt(\"%s/%d_%d.txt.gz\" % (data_prefix,m,g), delimiter=\"\\t\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "source": [
    "### 2.3 Define data options\n",
    "- **scale_groups**: if groups have different ranges/variances, it is good practice to scale each group to unit variance. Default is False\n",
    "- **scale_views**: if views have different ranges/variances, it is good practice to scale each view to unit variance. Default is False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-06T20:25:21.371541Z",
     "start_time": "2020-02-06T20:25:21.359823Z"
    }
   },
   "outputs": [],
   "source": [
    "ent.set_data_options(\n",
    "    scale_groups = False, \n",
    "    scale_views = False\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4) Add the data to the model\n",
    "\n",
    "This has to be run after defining the data options\n",
    "- **likelihods**: a list of strings, either \"gaussian\", \"poisson\" or \"bernoulli\". If None (default), they are guessed internally"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-06T20:25:21.862319Z",
     "start_time": "2020-02-06T20:25:21.805782Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "View names not provided, using default naming convention:\n",
      "- view1, view2, ..., viewM\n",
      "\n",
      "Features names not provided, using default naming convention:\n",
      "- feature1_view1, featureD_viewM\n",
      "\n",
      "Groups names not provided, using default naming convention:\n",
      "- group1, group2, ..., groupG\n",
      "\n",
      "Samples names not provided, using default naming convention:\n",
      "- sample1_group1, sample2_group1, sample1_group2, ..., sampleN_groupG\n",
      "\n",
      "Successfully loaded view='view0' group='group0' with N=100 samples and D=1000 features...\n",
      "Successfully loaded view='view0' group='group1' with N=100 samples and D=1000 features...\n",
      "Successfully loaded view='view1' group='group0' with N=100 samples and D=1000 features...\n",
      "Successfully loaded view='view1' group='group1' with N=100 samples and D=1000 features...\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# option 1: data.frame format (slower)\n",
    "ent.set_data_df(data_dt, likelihoods = [\"gaussian\",\"gaussian\"])\n",
    "\n",
    "# option 2: nested matrix format (faster)\n",
    "ent.set_data_matrix(data_mat, likelihoods = [\"gaussian\",\"gaussian\"])\n",
    "\n",
    "# AnnData format\n",
    "# (...)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3) Set model options\n",
    "\n",
    "- **factors**: number of factors\n",
    "- **spikeslab_weights**: use spike-slab sparsity prior in the weights? default is TRUE\n",
    "- **ard_factors**: use ARD prior in the factors? Default is TRUE if using multiple groups. This is ugessed by default.\n",
    "- **ard_weights**: use ARD prior in the weights? Default is TRUE if using multiple views. This is guessed by default\n",
    "\n",
    "Only change the default model options if you are familiar with the underlying mathematical model!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-06T20:25:26.955109Z",
     "start_time": "2020-02-06T20:25:26.914220Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model options:\n",
      "- Automatic Relevance Determination prior on the factors: True\n",
      "- Automatic Relevance Determination prior on the weights: True\n",
      "- Spike-and-slab prior on the factors: False\n",
      "- Spike-and-slab prior on the weights: True \n",
      "\n",
      "Likelihoods:\n",
      "- View 0 (view0): gaussian\n",
      "- View 1 (view1): gaussian\n"
     ]
    }
   ],
   "source": [
    "ent.set_model_options(\n",
    "    factors = 10, \n",
    "    spikeslab_weights = True, \n",
    "    ard_factors = True,\n",
    "    ard_weights = True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4) Set training options\n",
    "\n",
    "- **iter**: number of iterations. Default is 1000.\n",
    "- **convergence_mode**: \"fast\", \"medium\", \"slow\". For exploration, the fast mode is good enough.\n",
    "- **startELBO**: initial iteration to compute the ELBO (the objective function used to assess convergence)\n",
    "- **freqELBO**: frequency of computations of the ELBO (the objective function used to assess convergence)\n",
    "- **dropR2**: minimum variance explained criteria to drop factors while training\n",
    "gpu_mode: use GPU mode? (needs cupy installed and a functional GPU, see https://cupy.chainer.org/)\n",
    "- **verbose**: verbose mode?\n",
    "- **seed**: random seed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-06T20:25:29.519460Z",
     "start_time": "2020-02-06T20:25:29.509712Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "GPU mode is activated, but GPU not found... switching to CPU mode\n",
      "For GPU mode, you need:\n",
      "1 - Make sure that you are running MOFA+ on a machine with an NVIDIA GPU\n",
      "2 - Install CUPY following instructions on https://docs-cupy.chainer.org/en/stable/install.html\n",
      "\n"
     ]
    }
   ],
   "source": [
    "ent.set_train_options(\n",
    "    iter = 1000, \n",
    "    convergence_mode = \"fast\", \n",
    "    startELBO = 1, \n",
    "    freqELBO = 1, \n",
    "    dropR2 = 0.001, \n",
    "    gpu_mode = True, \n",
    "    verbose = False, \n",
    "    seed = 1\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 5) (Optional)  stochastic inference options\n",
    "\n",
    "If the number of samples is very large (at the order of >1e4), you may want to try the stochastic inference scheme. However, it requires some additional hyperparameters that in some data sets may need to be optimised (see the [stochastic vignette](https://raw.githack.com/bioFAM/MOFA2/master/MOFA2/vignettes/stochastic_inference.html)).\n",
    "\n",
    "- **batch_size**: float value indicating the batch size (as a fraction of the total data set: either 0.10, 0.25 or 0.50)\n",
    "- **learning_rate**: learning rate (we recommend values from 0.25 to 0.50)\n",
    "- **forgetting_rate**: forgetting rate (we recommend values from 0.75 to 1.0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We do not want to use stochastic inference for this data set\n",
    "\n",
    "# ent.set_stochastic_options(\n",
    "# batch_size = 0.5,\n",
    "# learning_rate = 0.5, \n",
    "# forgetting_rate = 0.25\n",
    "#)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6) Build and train the MOFA object \n",
    "\n",
    "After training, the model will be saved as an hdf5 file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-06T20:27:21.866240Z",
     "start_time": "2020-02-06T20:27:18.843347Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "######################################\n",
      "## Training the model with seed 1 ##\n",
      "######################################\n",
      "\n",
      "\n",
      "ELBO before training: -5381458.89 \n",
      "\n",
      "Iteration 1: time=0.06, ELBO=-575232.56, deltaELBO=4806226.336 (89.31084362%), Factors=9\n",
      "Iteration 2: time=0.07, ELBO=-475536.26, deltaELBO=99696.298 (1.85258867%), Factors=8\n",
      "Iteration 3: time=0.06, ELBO=-467044.81, deltaELBO=8491.451 (0.15779088%), Factors=7\n",
      "Iteration 4: time=0.05, ELBO=-462583.09, deltaELBO=4461.713 (0.08290899%), Factors=6\n",
      "Iteration 5: time=0.06, ELBO=-459116.18, deltaELBO=3466.915 (0.06442334%), Factors=5\n",
      "Iteration 6: time=0.04, ELBO=-458285.22, deltaELBO=830.956 (0.01544108%), Factors=5\n",
      "Iteration 7: time=0.04, ELBO=-457814.07, deltaELBO=471.158 (0.00875520%), Factors=5\n",
      "Iteration 8: time=0.04, ELBO=-457487.58, deltaELBO=326.485 (0.00606685%), Factors=5\n",
      "Iteration 9: time=0.06, ELBO=-457202.99, deltaELBO=284.587 (0.00528829%), Factors=5\n",
      "Iteration 10: time=0.04, ELBO=-456926.93, deltaELBO=276.062 (0.00512988%), Factors=5\n",
      "Iteration 11: time=0.05, ELBO=-456666.10, deltaELBO=260.835 (0.00484693%), Factors=5\n",
      "Iteration 12: time=0.11, ELBO=-456437.43, deltaELBO=228.665 (0.00424912%), Factors=5\n",
      "Iteration 13: time=0.08, ELBO=-456246.66, deltaELBO=190.774 (0.00354502%), Factors=5\n",
      "Iteration 14: time=0.04, ELBO=-456087.59, deltaELBO=159.067 (0.00295584%), Factors=5\n",
      "Iteration 15: time=0.05, ELBO=-455950.33, deltaELBO=137.263 (0.00255067%), Factors=5\n",
      "Iteration 16: time=0.06, ELBO=-455826.84, deltaELBO=123.490 (0.00229474%), Factors=5\n",
      "Iteration 17: time=0.04, ELBO=-455710.86, deltaELBO=115.979 (0.00215515%), Factors=5\n",
      "Iteration 18: time=0.05, ELBO=-455597.76, deltaELBO=113.095 (0.00210156%), Factors=5\n",
      "Iteration 19: time=0.05, ELBO=-455485.19, deltaELBO=112.574 (0.00209190%), Factors=5\n",
      "Iteration 20: time=0.05, ELBO=-455372.67, deltaELBO=112.518 (0.00209085%), Factors=5\n",
      "Iteration 21: time=0.06, ELBO=-455259.13, deltaELBO=113.542 (0.00210987%), Factors=5\n",
      "Iteration 22: time=0.05, ELBO=-455141.61, deltaELBO=117.515 (0.00218370%), Factors=5\n",
      "Iteration 23: time=0.05, ELBO=-455017.31, deltaELBO=124.306 (0.00230989%), Factors=5\n",
      "Iteration 24: time=0.05, ELBO=-454890.60, deltaELBO=126.711 (0.00235458%), Factors=5\n",
      "Iteration 25: time=0.06, ELBO=-454777.30, deltaELBO=113.294 (0.00210527%), Factors=5\n",
      "Iteration 26: time=0.07, ELBO=-454687.38, deltaELBO=89.924 (0.00167099%), Factors=5\n",
      "Iteration 27: time=0.08, ELBO=-454621.75, deltaELBO=65.626 (0.00121948%), Factors=5\n",
      "Iteration 28: time=0.06, ELBO=-454575.85, deltaELBO=45.908 (0.00085308%), Factors=5\n",
      "Iteration 29: time=0.24, ELBO=-454540.91, deltaELBO=34.939 (0.00064925%), Factors=5\n",
      "Iteration 30: time=0.06, ELBO=-454510.81, deltaELBO=30.094 (0.00055922%), Factors=5\n",
      "Iteration 31: time=0.06, ELBO=-454483.09, deltaELBO=27.720 (0.00051509%), Factors=5\n",
      "Iteration 32: time=0.07, ELBO=-454457.65, deltaELBO=25.444 (0.00047280%), Factors=5\n",
      "Iteration 33: time=0.09, ELBO=-454435.35, deltaELBO=22.304 (0.00041445%), Factors=5\n",
      "Iteration 34: time=0.04, ELBO=-454416.89, deltaELBO=18.454 (0.00034292%), Factors=5\n",
      "Iteration 35: time=0.06, ELBO=-454402.44, deltaELBO=14.450 (0.00026852%), Factors=5\n",
      "Iteration 36: time=0.06, ELBO=-454391.52, deltaELBO=10.926 (0.00020303%), Factors=5\n",
      "Iteration 37: time=0.04, ELBO=-454383.33, deltaELBO=8.189 (0.00015217%), Factors=5\n",
      "Iteration 38: time=0.05, ELBO=-454377.13, deltaELBO=6.200 (0.00011521%), Factors=5\n",
      "Iteration 39: time=0.06, ELBO=-454372.32, deltaELBO=4.807 (0.00008932%), Factors=5\n",
      "Iteration 40: time=0.06, ELBO=-454368.46, deltaELBO=3.858 (0.00007169%), Factors=5\n",
      "Iteration 41: time=0.04, ELBO=-454365.23, deltaELBO=3.230 (0.00006002%), Factors=5\n",
      "Iteration 42: time=0.05, ELBO=-454362.40, deltaELBO=2.828 (0.00005256%), Factors=5\n",
      "Iteration 43: time=0.05, ELBO=-454359.82, deltaELBO=2.585 (0.00004804%), Factors=5\n",
      "Iteration 44: time=0.05, ELBO=-454357.37, deltaELBO=2.452 (0.00004556%), Factors=5\n",
      "\n",
      "Converged!\n",
      "\n",
      "\n",
      "\n",
      "#######################\n",
      "## Training finished ##\n",
      "#######################\n",
      "\n",
      "\n",
      "Saving model in /Users/ricard/data/mofaplus/hdf5/test.hdf5...\n",
      "\n"
     ]
    }
   ],
   "source": [
    "ent.build()\n",
    "\n",
    "ent.run()\n",
    "\n",
    "# Save the output\n",
    "outfile = \"/Users/ricard/data/mofaplus/hdf5/test.hdf5\"\n",
    "ent.save(outfile)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7) Downstream analysis\n",
    "\n",
    "This finishes the tutorial on how to train a MOFA object from python. To continue with the downstream analysis you have to switch to R. Please, follow [this tutorial](https://raw.githack.com/bioFAM/MOFA2/master/MOFA2/vignettes/downstream_analysis.html)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  },
  "latex_metadata": {
   "affiliation": "European Bioinformatics Institute, Cambridge, UK",
   "author": "Ricard Argelaguet",
   "title": "MOFA+: training a model with Python"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "metadata": {
     "collapsed": false
    },
    "source": []
   }
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
