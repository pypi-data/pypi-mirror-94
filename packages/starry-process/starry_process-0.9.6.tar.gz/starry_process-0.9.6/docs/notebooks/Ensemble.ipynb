{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ensemble analyses"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this tutorial we'll show how to perform a very simple ensemble analysis to infer the statistical properties of the spots on a group of stars."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "hide_input"
    ]
   },
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "hide_input"
    ]
   },
   "outputs": [],
   "source": [
    "%config InlineBackend.figure_format = \"retina\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "hide_input"
    ]
   },
   "outputs": [],
   "source": [
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# Disable annoying font warnings\n",
    "matplotlib.font_manager._log.setLevel(50)\n",
    "\n",
    "# Disable theano deprecation warnings\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(\"ignore\", category=DeprecationWarning)\n",
    "warnings.filterwarnings(\"ignore\", category=matplotlib.MatplotlibDeprecationWarning)\n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning)\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning, module=\"theano\")\n",
    "\n",
    "# Style\n",
    "plt.style.use(\"default\")\n",
    "plt.rcParams[\"savefig.dpi\"] = 100\n",
    "plt.rcParams[\"figure.dpi\"] = 100\n",
    "plt.rcParams[\"figure.figsize\"] = (12, 4)\n",
    "plt.rcParams[\"font.size\"] = 14\n",
    "plt.rcParams[\"text.usetex\"] = False\n",
    "plt.rcParams[\"font.family\"] = \"sans-serif\"\n",
    "plt.rcParams[\"font.sans-serif\"] = [\"Liberation Sans\"]\n",
    "plt.rcParams[\"font.cursive\"] = [\"Liberation Sans\"]\n",
    "try:\n",
    "    plt.rcParams[\"mathtext.fallback\"] = \"cm\"\n",
    "except KeyError:\n",
    "    plt.rcParams[\"mathtext.fallback_to_cm\"] = True\n",
    "plt.rcParams[\"mathtext.fallback_to_cm\"] = True\n",
    "\n",
    "# Short arrays when printing\n",
    "np.set_printoptions(threshold=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "hide_input"
    ]
   },
   "outputs": [],
   "source": [
    "del matplotlib\n",
    "del plt\n",
    "del warnings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from starry_process import StarryProcess, calibrate\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm.auto import tqdm\n",
    "import pymc3 as pm\n",
    "import pymc3_ext as pmx\n",
    "from corner import corner\n",
    "import theano\n",
    "import theano.tensor as tt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "hide_input"
    ]
   },
   "outputs": [],
   "source": [
    "import matplotlib\n",
    "from corner import corner as _corner\n",
    "\n",
    "\n",
    "def corner(*args, **kwargs):\n",
    "    \"\"\"\n",
    "    Override `corner.corner` by making some appearance tweaks.\n",
    "\n",
    "    \"\"\"\n",
    "    # Get the usual corner plot\n",
    "    figure = _corner(*args, **kwargs)\n",
    "\n",
    "    # Get the axes\n",
    "    ndim = int(np.sqrt(len(figure.axes)))\n",
    "    axes = np.array(figure.axes).reshape((ndim, ndim))\n",
    "\n",
    "    # Smaller tick labels\n",
    "    for ax in axes[1:, 0]:\n",
    "        for tick in ax.yaxis.get_major_ticks():\n",
    "            tick.label.set_fontsize(10)\n",
    "        formatter = matplotlib.ticker.ScalarFormatter(useOffset=False)\n",
    "        ax.yaxis.set_major_formatter(formatter)\n",
    "        ax.set_ylabel(ax.get_ylabel(), fontsize=kwargs.get(\"corner_label_size\", 16))\n",
    "    for ax in axes[-1, :]:\n",
    "        for tick in ax.xaxis.get_major_ticks():\n",
    "            tick.label.set_fontsize(10)\n",
    "        formatter = matplotlib.ticker.ScalarFormatter(useOffset=False)\n",
    "        ax.xaxis.set_major_formatter(formatter)\n",
    "        ax.set_xlabel(ax.get_xlabel(), fontsize=kwargs.get(\"corner_label_size\", 16))\n",
    "\n",
    "    # Pad the axes to always include the truths\n",
    "    truths = kwargs.get(\"truths\", None)\n",
    "    if truths is not None:\n",
    "        for row in range(1, ndim):\n",
    "            for col in range(row):\n",
    "                lo, hi = np.array(axes[row, col].get_xlim())\n",
    "                if truths[col] < lo:\n",
    "                    lo = truths[col] - 0.1 * (hi - truths[col])\n",
    "                    axes[row, col].set_xlim(lo, hi)\n",
    "                    axes[col, col].set_xlim(lo, hi)\n",
    "                elif truths[col] > hi:\n",
    "                    hi = truths[col] - 0.1 * (hi - truths[col])\n",
    "                    axes[row, col].set_xlim(lo, hi)\n",
    "                    axes[col, col].set_xlim(lo, hi)\n",
    "\n",
    "                lo, hi = np.array(axes[row, col].get_ylim())\n",
    "                if truths[row] < lo:\n",
    "                    lo = truths[row] - 0.1 * (hi - truths[row])\n",
    "                    axes[row, col].set_ylim(lo, hi)\n",
    "                    axes[row, row].set_xlim(lo, hi)\n",
    "                elif truths[row] > hi:\n",
    "                    hi = truths[row] - 0.1 * (hi - truths[row])\n",
    "                    axes[row, col].set_ylim(lo, hi)\n",
    "                    axes[row, row].set_xlim(lo, hi)\n",
    "\n",
    "    return figure"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate the ensemble"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section we will generate a synthetic ensemble of light curves of stars with \"similar\" spot properties. Let's define some true values for the spot properties of the ensemble:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "truths = {\"r\": 15, \"mu\": 30, \"sigma\": 5, \"c\": 0.05, \"n\": 20}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "hide_input"
    ]
   },
   "outputs": [],
   "source": [
    "from IPython.display import display, Markdown\n",
    "from starry_process.defaults import defaults\n",
    "\n",
    "display(\n",
    "    Markdown(\n",
    "        \"\"\"\n",
    "| parameter | description | true value\n",
    "| - | :- | :-:\n",
    "| `r` | mean radius in degrees | `{r}`\n",
    "| `mu` | latitude distribution mode in degrees | `{mu}`\n",
    "| `sigma` | latitude distribution standard deviation in degrees | `{sigma}`\n",
    "| `c` | fractional spot contrast | `{c}`\n",
    "| `n` | number of spots | `{n}`\n",
    "\"\"\".format(\n",
    "            **truths\n",
    "        )\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's generate 500 light curves from stars at random inclinations with spots drawn from the distributions above.\n",
    "We'll do this by adding discrete circular spots to each star via the `starry_process.calibrate.generate`\n",
    "function.\n",
    "Note that in order to mimic real observations, we'll normalize each light curve to its mean value and subtract unity to get the \"relative\" flux.\n",
    "For simplicity, we'll give all of the light curves the same period and photometric uncertainty."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "raw_mimetype": "text/restructuredtext"
   },
   "source": [
    ".. note::\n",
    "\n",
    "    The ``starry_process.calibrate`` module is used internally to verify the calibration of the Gaussian process, so it's not very well documented. You can check out the list of valid keyword arguments to the ``generate`` function `here <https://github.com/rodluger/starry_process/blob/ba90c7e7ff9a89939ad35ad331404f050027805d/starry_process/calibrate/defaults.json>`_."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = calibrate.generate(\n",
    "    generate=dict(\n",
    "        normalized=True,\n",
    "        nlc=500,\n",
    "        period=1.0,\n",
    "        ferr=1e-3,\n",
    "        nspots=dict(mu=truths[\"n\"]),\n",
    "        radius=dict(mu=truths[\"r\"]),\n",
    "        latitude=dict(mu=truths[\"mu\"], sigma=truths[\"sigma\"]),\n",
    "        contrast=dict(mu=truths[\"c\"]),\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The variable `data` is a dictionary containing the light curves, the stellar maps (expressed as vectors of spherical harmonic coefficients `y`), plus some metadata."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t = data[\"t\"]\n",
    "flux = data[\"flux\"]\n",
    "ferr = data[\"ferr\"]\n",
    "y = data[\"y\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's visualize some of the light curves, all on the same scale:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(3, 5)\n",
    "for j, axis in enumerate(ax.flatten()):\n",
    "    axis.plot(t, flux[j] * 1000)\n",
    "    axis.set_ylim(-50, 50)\n",
    "    axis.set_xticks([0, 1, 2, 3, 4])\n",
    "    if j != 10:\n",
    "        axis.set_xticklabels([])\n",
    "        axis.set_yticklabels([])\n",
    "    else:\n",
    "        axis.set_xlabel(\"rotations\")\n",
    "        axis.set_ylabel(\"flux [ppt]\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the next section, we'll assume we observe only these 500 light curves. We do not know the inclinations of any of the stars or anything about their spot properties: only that all the stars have statistically similar spot distributions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inference"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's set up a simple probabilistic model using `pymc3` and solve for the five quantities above: the spot radius, the mode and standard deviation of the spot latitude, the spot contrast, and the number of spots. We'll place uniform priors on everything except for the latitude mode `mu`, on which we'll place an isotropic prior."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with pm.Model() as model:\n",
    "\n",
    "    # For use later\n",
    "    varnames = [\"r\", \"mu\", \"sigma\", \"c\", \"n\"]\n",
    "\n",
    "    # Spot latitude params. Isotropic prior on the mode\n",
    "    # and uniform prior on the standard deviation\n",
    "    u = pm.Uniform(\"u\", 0.0, 1.0)\n",
    "    mu = 90 - tt.arccos(u) * 180 / np.pi\n",
    "    pm.Deterministic(\"mu\", mu)\n",
    "    sigma = pm.Uniform(\"sigma\", 1.0, 20.0)\n",
    "\n",
    "    # Spot radius (uniform prior)\n",
    "    r = pm.Uniform(\"r\", 10.0, 30.0)\n",
    "\n",
    "    # Spot contrast & number of spots (uniform prior)\n",
    "    c = pm.Uniform(\"c\", 0.0, 1.0, testval=0.1)\n",
    "    n = pm.Uniform(\"n\", 1.0, 50.0, testval=5)\n",
    "\n",
    "    # Instantiate the GP\n",
    "    sp = StarryProcess(r=r, mu=mu, sigma=sigma, c=c, n=n)\n",
    "\n",
    "    # Compute the log likelihood\n",
    "    lnlike = sp.log_likelihood(t, flux, ferr ** 2, p=1.0)\n",
    "    pm.Potential(\"lnlike\", lnlike)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "raw_mimetype": "text/restructuredtext"
   },
   "source": [
    ".. note::\n",
    "\n",
    "   Note that we explicitly provide a small ``testval`` for both the spot contrast and the number of spots. Otherwise, the initial value assumed in the optimization for both those quantities is the midpoint of the bounds (`c = 0.5` and `n = 25.5`). That corresponds to a *lot* of *very dark* spots, which results in very high variance in the flux -- too high, in fact, for the normalized Gaussian process to model! We discusss this in more detail in the paper. To avoid initializing the sampler in a bad region of parameter space, we provide a starting point that is guaranteed to lead to a finite log likelihood value."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We could go on to do inference using `NUTS` or `ADVI` or any of the other samplers supported by `pymc3`. But that would take a few hours (at least). Since we have many light curves,let's just optimize the log probability function to get the MAP (maximum a posteriori) solution -- that will be a good estimate of the true spot properties."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "map_soln = pmx.optimize(model=model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here's what we got:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "hide_input"
    ]
   },
   "outputs": [],
   "source": [
    "from IPython.display import display, Markdown\n",
    "from starry_process.defaults import defaults\n",
    "\n",
    "display(\n",
    "    Markdown(\n",
    "        \"\"\"\n",
    "| parameter | description | true value | inferred value\n",
    "| - | :- | :-: | :-:\n",
    "| `r` | mean radius in degrees | `{r}` | `{{r:.2f}}`\n",
    "| `mu` | latitude distribution mode in degrees | `{mu}` | `{{mu:.2f}}`\n",
    "| `sigma` | latitude distribution standard deviation in degrees | `{sigma}` | `{{sigma:.2f}}`\n",
    "| `c` | fractional spot contrast | `{c}` | `{{c:.4f}}`\n",
    "| `n` | number of spots | `{n}` | `{{n:.2f}}`\n",
    "\"\"\".format(\n",
    "            **truths\n",
    "        ).format(\n",
    "            **map_soln\n",
    "        )\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Not bad! We correctly inferred *all* the hyperparameters of the GP! Note, importantly, that we don't yet have any estimate of the uncertainty on any of these parameters. To get that, we need to actually sample the posterior. Stay tuned!"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "raw_mimetype": "text/restructuredtext"
   },
   "source": [
    ".. warning::\n",
    "\n",
    "    Note that our inferred value for the number of spots is pretty close to the true value. But as we will see, the uncertainty is very large! In general, it's extremely difficult to constrain the total number of spots from stellar light curves."
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Raw Cell Format",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
