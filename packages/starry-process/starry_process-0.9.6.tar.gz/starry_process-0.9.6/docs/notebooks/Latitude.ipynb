{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inferring spot latitudes "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the [Quickstart](Quickstart.ipynb) tutorial, we showed how to infer the radii of the star spots in a (very simple) example. In this notebook we'll discuss how to infer the parameters controlling their distribution in latitude."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "hide_input"
    ]
   },
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "hide_input"
    ]
   },
   "outputs": [],
   "source": [
    "%config InlineBackend.figure_format = \"retina\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "hide_input"
    ]
   },
   "outputs": [],
   "source": [
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# Disable annoying font warnings\n",
    "matplotlib.font_manager._log.setLevel(50)\n",
    "\n",
    "# Disable theano deprecation warnings\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(\"ignore\", category=DeprecationWarning)\n",
    "warnings.filterwarnings(\"ignore\", category=matplotlib.MatplotlibDeprecationWarning)\n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning)\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning, module=\"theano\")\n",
    "\n",
    "# Style\n",
    "plt.style.use(\"default\")\n",
    "plt.rcParams[\"savefig.dpi\"] = 100\n",
    "plt.rcParams[\"figure.dpi\"] = 100\n",
    "plt.rcParams[\"figure.figsize\"] = (12, 4)\n",
    "plt.rcParams[\"font.size\"] = 14\n",
    "plt.rcParams[\"text.usetex\"] = False\n",
    "plt.rcParams[\"font.family\"] = \"sans-serif\"\n",
    "plt.rcParams[\"font.sans-serif\"] = [\"Liberation Sans\"]\n",
    "plt.rcParams[\"font.cursive\"] = [\"Liberation Sans\"]\n",
    "try:\n",
    "    plt.rcParams[\"mathtext.fallback\"] = \"cm\"\n",
    "except KeyError:\n",
    "    plt.rcParams[\"mathtext.fallback_to_cm\"] = True\n",
    "plt.rcParams[\"mathtext.fallback_to_cm\"] = True\n",
    "\n",
    "# Short arrays when printing\n",
    "np.set_printoptions(threshold=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "hide_input"
    ]
   },
   "outputs": [],
   "source": [
    "del matplotlib\n",
    "del plt\n",
    "del warnings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's import some basic stuff. We'll use the `emcee` package to run our MCMC analyses below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from starry_process import StarryProcess, beta2gauss\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm.auto import tqdm\n",
    "import theano\n",
    "import theano.tensor as tt\n",
    "import emcee\n",
    "from corner import corner"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll instantiate a `StarryProcess` with high-latitude spots at $\\phi = 60^\\circ \\pm 5^\\circ$:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "truths = [60.0, 5.0]\n",
    "sp = StarryProcess(mu=truths[0], sigma=truths[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here's what a random sample from the process looks like on the surface of the star:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sp.visualize()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To start us off, let's draw 50 light curve samples from the process:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t = np.linspace(0, 4, 500)\n",
    "flux = sp.sample(t, nsamples=50).eval()\n",
    "flux.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All samples have the same rotation period (`1.0` day by default), and we're computing the light curves over 4 rotations. Since by default `StarryProcess` marginalizes over inclination, these samples correspond to random inclinations drawn from an isotropic distribution. Let's visualize all of them:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for k in range(50):\n",
    "    plt.plot(t, 1e3 * flux[k], alpha=0.5)\n",
    "plt.xlabel(\"rotations\")\n",
    "plt.ylabel(\"relative flux [ppt]\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since we're going to do inference, we need to add a bit of observational noise to these samples to mimic actual observations. Here's what the first light curve looks like:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ferr = 1e-3\n",
    "np.random.seed(0)\n",
    "f = flux + ferr * np.random.randn(50, len(t))\n",
    "plt.plot(t, flux[0], \"C0-\", lw=0.75, alpha=0.5)\n",
    "plt.plot(t, f[0], \"C0.\", ms=3)\n",
    "plt.xlabel(\"time [days]\")\n",
    "plt.ylabel(\"relative flux [ppt]\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MCMC"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given these 50 light curves, our task now is to infer the mean and standard deviation of the spot latitude distribution. For simplicity, we'll assume we know everything else: the spot radius, contrast, number of spots, and stellar rotation period (but not the inclinations). That makes the inference problem 2-dimensional and thus fast to run."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "raw_mimetype": "text/restructuredtext"
   },
   "source": [
    ".. note::\n",
    "    \n",
    "    In two dimensions, it's usually more efficient to grid up the space, compute the likelihood everywhere on the grid, then sum it up to find the normalization constant directly and thus obtain the posterior. But since most use cases will be > 2d, we'll do inference here using MCMC as an example."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A few notes about this. First, the spot latitude distribution assumed internally is *not* a Gaussian. It's a Beta distribution in the cosine of the latitude, which in certain limits *looks* a lot like a bimodal Gaussian, with a mode at $+\\mu_\\sigma$ and a mode at $-\\mu_\\sigma$. The parameters of this distribution are the *mode* `mu` and the local standard deviation at the mode *sigma*, which in many cases are good approximations to the mean and standard deviation of the closest-matching Gaussian. The reason for this is explained in the paper: the adopted distribution and parametrization admits a **closed-form solution** to the mean and covariance of the GP, which is at the heart of why `starry_process` is so fast.\n",
    "\n",
    "For reference, you can compute the actual distribution assumed internally by calling\n",
    "\n",
    "```python\n",
    "sp.latitude.pdf(phi)\n",
    "```\n",
    "\n",
    "where `sp` is an instance of `StarryProcess` (with certain values of `mu` and `sigma`), and `phi` is the latitude(s) in degrees at which to compute the probability density function (PDF). Here's the distribution we used to generate the light curve samples:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "phi = np.linspace(-90, 90, 1000)\n",
    "pdf = sp.latitude.pdf(phi).eval()\n",
    "plt.plot(phi, pdf)\n",
    "plt.xlabel(\"latitude [degrees]\")\n",
    "plt.ylabel(\"probability density\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As promised, it looks a *lot* like a bimodal Gaussian with mean `mu = 60` and standard deviation `sigma = 5`. Typically, this distribution deviates from a Gaussian when either `mu` or `sigma` get very large."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The other thing to note before we do inference is that there are two ways of proceeding. We can either sample in the parameters `mu` and `sigma` *or* in the dimensionless parameters `a` and `b`. In most cases we'll get the same answer (provided we account for the Jacobian of the transformation), but we recommend the latter as it's guaranteed to be numerically stable. That's because there are certain combinations of `mu` and `sigma` that lead to *very* large coefficients in the evaluation of the integrals of the Beta distribution. This usually happens when `sigma` is very small (less than 1 or 2 degrees) or `mu` is very large (larger than 85 degrees). Sometimes this can raise errors in the likelihood evaluation (which can be caught in a `try...except` block), but sometimes it can just silently lead to the wrong value of the likelihood. The latter can be avoided by limiting the prior bounds on these two quantities. We do this below, and show that for this specific example, it works great! \n",
    "\n",
    "However, a better approach is to sample in the dimensionless parameters `a` and `b`, which both have support in `[0, 1]`, then transform the posterior constraints on these parameters into constraints on `mu` and `sigma`. Sampling in this space is much, much more likely to work seamlessly. We'll discuss how to do this below. But first, let's show how to sample in `mu` and `sigma` directly."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sampling in `mu`, `sigma`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As in the [Quickstart](Quickstart.ipynb) tutorial, we need to compile our likelihood function. We'll make it a function of the scalars `mu` and `sigma`, and bake in the time array `t`, the batch of light curves `f`, and the photometric uncertainty `ferr` (as well as all the other default hyperparameter values)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mu_tensor = tt.dscalar()\n",
    "sigma_tensor = tt.dscalar()\n",
    "log_likelihood1 = theano.function(\n",
    "    [mu_tensor, sigma_tensor],\n",
    "    StarryProcess(mu=mu_tensor, sigma=sigma_tensor).log_likelihood(t, f, ferr ** 2),\n",
    ")"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "raw_mimetype": "text/restructuredtext"
   },
   "source": [
    ".. note::\n",
    "\n",
    "    In some of the other tutorials, we compute the joint log likelihood of all the light curves in an ensemble by summing the log likelihood of each one. This requires as many function calls as there are light curves (50 in this case), which can lead to painfully slow runtimes. We can drastically improve on this by passing in all the light curves at once as a matrix whose rows are individual light curves, which will re-use the factorization of the covariance matrix for all of them. However, this assumes that all stars have the same hyperparameters, *including* the rotation period, limb darkening coefficients, and the photometric uncertainty (but not the inclination, since we're marginalizing over it). In this particular example, all stars have the same period and photometric uncertainty (and no limb darkening), so we can get away with this shortcut. But in general, this will not be the correct approach!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now construct our log probability function (the sum of the log likelihood and the log prior). Let's adopt the simplest possible prior: a uniform distribution for `mu` and `sigma` that avoids the unstable values discssued above. We'll draw from this prior to obtain our initial sampling point, `p0`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def log_prob1(x):\n",
    "    mu, sigma = x\n",
    "    if (mu < 0) or (mu > 85):\n",
    "        return -np.inf\n",
    "    elif (sigma < 1) or (sigma > 30):\n",
    "        return -np.inf\n",
    "    return log_likelihood1(mu, sigma)\n",
    "\n",
    "\n",
    "ndim, nwalkers = 2, 6\n",
    "p0 = np.transpose(\n",
    "    [np.random.uniform(0, 85, size=nwalkers), np.random.uniform(1, 30, size=nwalkers)]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can instantiate a sampler (using `emcee`) and run it. We'll do 1000 steps with 6 walkers:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sampler1 = emcee.EnsembleSampler(nwalkers, ndim, log_prob1)\n",
    "_ = sampler1.run_mcmc(p0, 1000, progress=True)\n",
    "chain1 = np.array(sampler1.chain)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is the trace plot:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(2)\n",
    "for k in range(nwalkers):\n",
    "    ax[0].plot(chain1[k, :, 0])\n",
    "    ax[1].plot(chain1[k, :, 1])\n",
    "ax[0].set_ylabel(r\"$\\mu$ [deg]\")\n",
    "ax[1].set_ylabel(r\"$\\sigma$ [deg]\")\n",
    "ax[1].set_xlabel(\"iteration\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This has *definitely* not converged, but it's good enough for our toy example. Let's remove the first 100 samples as our burn-in and plot the joint posterior:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corner(chain1[:, 100:, :].reshape(-1, 2), truths=truths, labels=[\"$\\mu$\", \"$\\sigma$\"])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Not bad! The true values are indicated in blue. We correctly infer the presence of high-latitude spots! Note, however, the degeneracy here. It's difficult to rule out polar spots with high variance! We discuss this at length in the paper. Pay attention to the curvature of the degeneracy, which gets worse for higher latitude spots. In some cases, this might be a very hard posterior to sample using MCMC!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sampling in `a`, `b`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we mentioned above, it's preferable to sample in the dimensionless parameters `a` and `b`, which are just a (nonlinear) transformation of `mu` and `sigma` onto the unit square. There is a one-to-one correspondence between the two sets of quantities. Converting between them can be done via the utility functions\n",
    "\n",
    "```python\n",
    "mu, sigma = starry_process.beta2gauss(a, b)\n",
    "```\n",
    "\n",
    "and\n",
    "\n",
    "```python\n",
    "a, b = starry_process.gauss2beta(mu, sigma)\n",
    "```\n",
    "\n",
    "Let's compile our likelihood function to take `a` and `b` as input:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a_tensor = tt.dscalar()\n",
    "b_tensor = tt.dscalar()\n",
    "sp = StarryProcess(a=a_tensor, b=b_tensor)\n",
    "log_likelihood2 = theano.function(\n",
    "    [a_tensor, b_tensor],\n",
    "    sp.log_likelihood(t, f, ferr ** 2) + sp.log_jac(),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that we added an extra term in the computation of the likelihood: `sp.log_jac()`. This is the log of the absolute value of the determinant of the Jacobian of the transformation (ugh!), which corrects for the (very wonky) implicit prior introduced by our reparametrization. By adding this term, we can place a uniform prior on `a` and `b` in `[0, 1]` and effectively obtain a uniform prior on `mu` and `sigma` when we transform our posterior constraints below.\n",
    "\n",
    "Here's our log probability function, with the trivial prior on `a` and `b`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def log_prob2(x):\n",
    "    a, b = x\n",
    "    if (a < 0) or (a > 1):\n",
    "        return -np.inf\n",
    "    elif (b < 0) or (b > 1):\n",
    "        return -np.inf\n",
    "    return log_likelihood2(a, b)\n",
    "\n",
    "\n",
    "ndim, nwalkers = 2, 6\n",
    "p0 = np.random.uniform(0, 1, size=(nwalkers, 2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's run the sampler as before:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sampler2 = emcee.EnsembleSampler(nwalkers, ndim, log_prob2)\n",
    "_ = sampler2.run_mcmc(p0, 1000, progress=True)\n",
    "chain2_ab = np.array(sampler2.chain)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our samples are in the dimensionless quantities `a` and `b`, but what we really want are samples in `mu` and `sigma`. Let's use the utility function mentioned above to transform them:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chain2 = np.zeros_like(chain2_ab)\n",
    "for k in range(nwalkers):\n",
    "    for n in range(1000):\n",
    "        chain2[k, n] = beta2gauss(*chain2_ab[k, n])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here's the trace plot:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(2)\n",
    "for k in range(nwalkers):\n",
    "    ax[0].plot(chain2[k, :, 0])\n",
    "    ax[1].plot(chain2[k, :, 1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "and, finally, the joint posterior:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corner(chain2[:, 100:, :].reshape(-1, 2), truths=truths, labels=[\"$\\mu$\", \"$\\sigma$\"])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As expected, we get very similar results (keeping in mind the chains are still very far from convergence!)\n",
    "\n",
    "Bottom line: you can sample in whichever parametrization you want, but we recommend using `a` and `b` as your parameters, as long as you remember to add the Jacobian term to the likelihood."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "raw_mimetype": "text/restructuredtext"
   },
   "source": [
    ".. note::\n",
    "\n",
    "    If you're explicitly summing log-likelihoods in an ensemble analysis, remember that you only want to add the log Jacobian a *single* time!"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Raw Cell Format",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
