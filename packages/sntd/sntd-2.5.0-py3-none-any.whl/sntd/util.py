import os
import sncosmo
import glob
import sys
import subprocess
import time
import scipy
import tarfile
import pickle
import matplotlib.pyplot as plt
import numpy as np
from collections import OrderedDict as odict
from astropy.io import ascii
from scipy.interpolate import splrep, splev
from copy import copy
from scipy.stats import rv_continuous


_current_dir_ = os.path.abspath(os.getcwd())
_filedir_ = os.path.abspath(os.path.dirname(__file__))

NORMAL = 0    # use python zip libraries
PROCESS = 1   # use (zcat, gzip) or (bzcat, bzip2)
PARALLEL = 2  # (pigz -dc, pigz) or (pbzip2 -dc, pbzip2)


_props = odict([
    ('time', {'mjd', 'mjdobs', 'jd', 'time', 'date', 'mjd_obs', 'mhjd', 'jds'}),
    ('band', {'filter', 'band', 'flt', 'bandpass'}),
    ('flux', {'flux', 'f', 'fluxes'}),
    ('fluxerr', {'flux_error', 'fluxerr',
                 'fluxerror', 'fe', 'flux_err', 'fluxerrs'}),
    ('zp', {'zero_point', 'zp', 'zpt', 'zeropoint'}),
    ('zpsys', {'zpsys', 'magsys', 'zpmagsys'}),
    ('mag', {'mag', 'magnitude', 'mags'}),
    ('magerr', {'magerr', 'magerror',
                'magnitudeerror', 'magnitudeerr', 'magerrs'})
])

_sncosmo_snana = [('snana-2004fe', 'SN Ic', 'CSP-2004fe.SED'),
                  ('snana-2004gq', 'SN Ic', 'CSP-2004gq.SED'),
                  ('snana-sdss004012', 'SN Ic', 'SDSS-004012.SED'),  # no IAU name
                  ('snana-2006fo', 'SN Ic', 'SDSS-013195.SED'),  # PSNID
                  ('snana-sdss014475', 'SN Ic', 'SDSS-014475.SED'),  # no IAU name
                  ('snana-2006lc', 'SN Ic', 'SDSS-015475.SED'),
                  # type Ic in SNANA
                  ('snana-2007ms', 'SN II-pec', 'SDSS-017548.SED'),
                  ('snana-04d1la', 'SN Ic', 'SNLS-04D1la.SED'),
                  ('snana-04d4jv', 'SN Ic', 'SNLS-04D4jv.SED'),
                  ('snana-2004gv', 'SN Ib', 'CSP-2004gv.SED'),
                  ('snana-2006ep', 'SN Ib', 'CSP-2006ep.SED'),
                  ('snana-2007Y', 'SN Ib', 'CSP-2007Y.SED'),
                  ('snana-2004ib', 'SN Ib', 'SDSS-000020.SED'),
                  ('snana-2005hm', 'SN Ib', 'SDSS-002744.SED'),  # PSNID
                  ('snana-2006jo', 'SN Ib', 'SDSS-014492.SED'),  # PSNID
                  ('snana-2007nc', 'SN Ib', 'SDSS-019323.SED'),
                  ('snana-2004hx', 'SN IIP', 'SDSS-000018.SED'),  # PSNID
                  ('snana-2005gi', 'SN IIP', 'SDSS-003818.SED'),  # PSNID
                  ('snana-2006gq', 'SN IIP', 'SDSS-013376.SED'),
                  ('snana-2006kn', 'SN IIP', 'SDSS-014450.SED'),
                  ('snana-2006jl', 'SN IIP', 'SDSS-014599.SED'),  # PSNID
                  ('snana-2006iw', 'SN IIP', 'SDSS-015031.SED'),
                  ('snana-2006kv', 'SN IIP', 'SDSS-015320.SED'),
                  ('snana-2006ns', 'SN IIP', 'SDSS-015339.SED'),
                  ('snana-2007iz', 'SN IIP', 'SDSS-017564.SED'),
                  ('snana-2007nr', 'SN IIP', 'SDSS-017862.SED'),
                  ('snana-2007kw', 'SN IIP', 'SDSS-018109.SED'),
                  ('snana-2007ky', 'SN IIP', 'SDSS-018297.SED'),
                  ('snana-2007lj', 'SN IIP', 'SDSS-018408.SED'),
                  ('snana-2007lb', 'SN IIP', 'SDSS-018441.SED'),
                  ('snana-2007ll', 'SN IIP', 'SDSS-018457.SED'),
                  ('snana-2007nw', 'SN IIP', 'SDSS-018590.SED'),
                  ('snana-2007ld', 'SN IIP', 'SDSS-018596.SED'),
                  ('snana-2007md', 'SN IIP', 'SDSS-018700.SED'),
                  ('snana-2007lz', 'SN IIP', 'SDSS-018713.SED'),
                  ('snana-2007lx', 'SN IIP', 'SDSS-018734.SED'),
                  ('snana-2007og', 'SN IIP', 'SDSS-018793.SED'),
                  ('snana-2007ny', 'SN IIP', 'SDSS-018834.SED'),
                  ('snana-2007nv', 'SN IIP', 'SDSS-018892.SED'),
                  ('snana-2007pg', 'SN IIP', 'SDSS-020038.SED'),
                  ('snana-2006ez', 'SN IIn', 'SDSS-012842.SED'),
                  ('snana-2006ix', 'SN IIn', 'SDSS-013449.SED'),
                  ('s11-2004hx', 'SN IIL/P', 'S11_SDSS-000018.SED'),
                  ('s11-2005lc', 'SN IIP', 'S11_SDSS-001472.SED'),
                  ('s11-2005hl', 'SN Ib', 'S11_SDSS-002000.SED'),
                  ('s11-2005hm', 'SN Ib', 'S11_SDSS-002744.SED'),
                  ('s11-2005gi', 'SN IIP', 'S11_SDSS-003818.SED'),
                  ('s11-2006fo', 'SN Ic', 'S11_SDSS-013195.SED'),
                  ('s11-2006jo', 'SN Ib', 'S11_SDSS-014492.SED'),
                  ('s11-2006jl', 'SN IIP', 'S11_SDSS-014599.SED')]


class newDict(dict):
    """
    This is just a dictionary replacement class that allows the use of a normal dictionary but with the ability
    to access via "dot" notation.
    """

    def __init__(self):
        super(newDict, self).__init__()

    # these three functions allow you to access the dict via "dot" notation
    __setattr__ = dict.__setitem__
    __delattr__ = dict.__delitem__
    __getattr__ = dict.__getitem__

    def _getstate_(self):
        """
        A function necessary for pickling
        :return: self
        """
        return self

    def _setstate_(self, d):
        """
        A function necessary for pickling
        :param d: A value
        :return: self._dict_
        """
        self.__dict__ = d

def get_models_by_sntype(snType):
    mod, types = np.loadtxt(os.path.join(
        _filedir_, 'data', 'sncosmo', 'models.ref'), dtype='str', unpack=True)
    modDict = {mod[i]: types[i] for i in range(len(mod))}
    mods = [x[0] for x in sncosmo.models._SOURCES._loaders.keys(
    ) if x[0] in modDict.keys() and modDict[x[0]][:len(snType)] == snType]
    return(mods)


def snana_to_sncosmo(snana_mod):
    for sncosmo_name, typ, snana_name in _sncosmo_snana:
        if snana_name == snana_mod:
            return[typ, sncosmo_name]
    return None


def sncosmo_to_snana(sncosmo_mod):
    for sncosmo_name, typ, snana_name in _sncosmo_snana:
        if sncosmo_name == sncosmo_mod:
            return[typ, snana_name]
    return None


def load_example_data():
    example_files = glob.glob(os.path.join(
        _filedir_, 'data', 'examples', '*.dat'))
    return(ascii.read(example_files[0]), ascii.read(example_files[1]))


def load_example_misn():
    example_file = glob.glob(os.path.join(
        _filedir_, 'data', 'examples', '*.pkl'))
    with open(example_file[0], 'rb') as f:
        misn = pickle.load(f)

    return(misn)


def load_batch_fit_names(folder_name='.', verbose=True):
    tars = glob.glob(os.path.join(folder_name, 'sntd_fits*.tar.gz'))
    all_names = {}
    for tar_fname in tars:
        tar = tarfile.open(tar_fname, 'r')
        all_fits = tar.getmembers()
        if verbose:
            print('Found %i fits, loading...' % len(all_fits))
        tar.close()
        all_names[tar_fname] = [x.name for x in all_fits]
    return all_names


def load_batch_fit(fit_name, folder=None, tar_dict=None):
    if tar_dict is None:
        folder = '.' if folder is None else folder
        tar_dict = load_batch_fit_names(folder)
    to_return = None
    for tar_fname in tar_dict.keys():
        if fit_name not in tar_dict[tar_fname]:
            continue

        tar = tarfile.open(tar_fname, 'r')
        f = tar.extractfile(fit_name).read()
        dat = pickle.loads(f)
        tar.close()
        return dat
    print('Did not find your file')
    return


def check_table_quality(table, min_n_bands=1, min_n_points_per_band=1, clip=False):
    ngood_bands = 0
    for b in np.unique(table['band']):
        temp_n_for_b = len(table[table['band'] == b])
        if temp_n_for_b < min_n_points_per_band:
            if clip:
                table = table[table['band'] != b]
        else:
            ngood_bands += 1
    if ngood_bands < min_n_bands:
        return table, False
    return table, True


def run_sbatch(folder_name, script_name_init, script_name, total_jobs, max_batch_jobs, n_per_node, wait_for_batch, parallelize, ncurves, verbose):
    fits_output = tarfile.open(os.path.join(
        os.path.abspath(folder_name), 'sntd_fits.tar.gz'), mode='w')

    result = subprocess.call(['sbatch', os.path.join(os.path.abspath(folder_name),
                                                     script_name_init)])
    if wait_for_batch:
        printProgressBar(0, total_jobs)
    ndone = 0
    nadded = min(total_jobs, max_batch_jobs)
    saved_fits = 0
    tarfit_ind = 0
    if parallelize is not None:
        n_per_file = 1
    else:
        n_per_file = n_per_node

    while True:
        time.sleep(10)  # update every 10 seconds
        done_files = glob.glob(os.path.join(
            os.path.abspath(folder_name), 'sntd_fit*.DONE'))
        if len(done_files) == 0:
            continue
        done_file = done_files[0]
        done = str(np.loadtxt(done_file, dtype=str))

        while done == 'FALSE':
            time.sleep(10)  # update every 10 seconds
            done = str(np.loadtxt(done_file, dtype=str))

        os.remove(done_file)
        output = glob.glob(done_file[:-5]+'*.pkl')
        saved_fits += len(output)

        if len(output) > 0:
            if int(saved_fits*n_per_file) >= 50000*(tarfit_ind+1):
                fits_output.close()
                fits_output = tarfile.open(os.path.join(os.path.abspath(
                    folder_name), 'sntd_fits_%i.tar.gz' % tarfit_ind), mode='w')
                tarfit_ind += 1
            for filename in output:
                fits_output.add(filename)
                os.remove(filename)

            if nadded < total_jobs:
                for i in range(math.ceil(len(output)/(n_per_node/n_per_file))):
                    if nadded > total_jobs-1:
                        continue
                    result = subprocess.call(['sbatch', os.path.join(os.path.abspath(folder_name),
                                                                     script_name), str(nadded)], stdout=subprocess.DEVNULL)
                    nadded += 1

            if wait_for_batch:
                printProgressBar(
                    saved_fits/(n_per_node/n_per_file), total_jobs)
        if saved_fits >= ncurves:
            break
    fits_output.close()
    if verbose:
        print('Done!')
    return


def make_sbatch(partition=None, njobs=None, njobstotal=None, python_path=None, init=False, folder=None, parallelize=None, microlensing_cores=None):
    if njobs is None:
        print("Batch mode requires a number of jobs!")
        sys.exit(1)
    if init:
        if njobstotal is None:
            print("Batch mode requires a total number of jobs!")
        n = 0
        add = ''
        done = False
        while not done:
            try:
                folder_name = 'batch_output%s' % add
                os.mkdir(folder_name)
                done = True
            except:
                add = str(n)
                n += 1
            if n > 50:
                print('Having trouble making batch output folder.')
                sys.exit(1)
    else:
        folder_name = folder

    if python_path is None:
        python_path = subprocess.check_output(
            "which python", shell=True).decode('utf-8').strip('\n')
    if not init:
        with open(os.path.join(_filedir_, 'batch', 'sbatch_job.BATCH')) as f:
            sbatch = f.read()
        if parallelize is None:
            pyfile = 'run_sntd.py'
        else:
            pyfile = 'run_sntd_par.py'
    else:
        with open(os.path.join(_filedir_, 'batch', 'sbatch_job_init.BATCH')) as f:
            sbatch = f.read()
        if parallelize is None:
            pyfile = 'run_sntd_init.py'
        else:
            pyfile = 'run_sntd_init_par.py'

    sbatch = sbatch.replace(
        'pyjob%j.out', os.path.join(folder_name, 'pyjob%j.out'))
    if partition is not None:
        sbatch = sbatch.replace('partition', '#SBATCH -p %s' % partition)
    else:
        sbatch = sbatch.replace('partition', '')
    sbatch = sbatch.replace('myPython', python_path)
    sbatch = sbatch.replace('run_sntd.py', os.path.join(
        os.path.abspath(folder_name), pyfile))
    if init:
        sbatch = sbatch.replace('njobstotal', '0-%i' % (njobstotal-1))
        sbatch = sbatch.replace('njobs', '%i' % njobs)
    if parallelize is not None:
        sbatch = sbatch.replace('ncores', str(parallelize))
    elif microlensing_cores is not None:
        sbatch = sbatch.replace('ncores', str(microlensing_cores))
    else:
        sbatch = sbatch.replace('ncores', str(1))

    if not init:
        with open(os.path.join(folder_name, 'sbatch_job.BATCH'), 'w') as f:
            f.write(sbatch)
        return('sbatch_job.BATCH', folder_name)
    else:
        with open(os.path.join(folder_name, 'sbatch_job_init.BATCH'), 'w') as f:
            f.write(sbatch)
        return('sbatch_job_init.BATCH', folder_name)


def plot(plot_type, x, y=None, yerr=None, xerr=None, ax=None, x_lab='', y_lab='', fontsize=18, figsize=(12, 12),
         x_name=None, y_name=None, label_name=None, **kwargs):
    if ax is None and plot_type != 'joint':
        fig = plt.figure(figsize=figsize)
        ax = fig.gca()

    if plot_type == 'scatter':
        ax.scatter(x, y, **kwargs)
    elif plot_type == 'plot':
        ax.plot(x, y, **kwargs)
    elif plot_type == 'errorbar':
        ax.errorbar(x, y, xerr=xerr, yerr=yerr, **kwargs)
    elif plot_type == 'hist':
        ax.hist(x, **kwargs)
    elif plot_type == 'joint':
        g = multivariateGrid(x_name, y_name, label_name, df=x)
        fig = g.ax_joint.__dict__['figure']
        ax = fig.gca()
        fig.set_size_inches(figsize[0], figsize[1])
    else:
        raise RuntimeError('What plot are you trying to do.')
    for tick in ax.xaxis.get_major_ticks():
        tick.label.set_fontsize(16)
    for tick in ax.yaxis.get_major_ticks():
        tick.label.set_fontsize(16)
    ax.set_xlabel(x_lab, fontsize=fontsize)
    ax.set_ylabel(y_lab, fontsize=fontsize)
    return(ax)


def printProgressBar(iteration, total, prefix='', suffix='', decimals=1, length=100, fill='█', printEnd="\r"):
    """
    Call in a loop to create terminal progress bar
    @params:
        iteration   - Required  : current iteration (Int)
        total       - Required  : total iterations (Int)
        prefix      - Optional  : prefix string (Str)
        suffix      - Optional  : suffix string (Str)
        decimals    - Optional  : positive number of decimals in percent complete (Int)
        length      - Optional  : character length of bar (Int)
        fill        - Optional  : bar fill character (Str)
        printEnd    - Optional  : end character (e.g. "\r", "\r\n") (Str)
    """
    percent = ("{0:." + str(decimals) + "f}").format(100 *
                                                     (iteration / float(total)))
    filledLength = int(length * iteration // total)
    bar = fill * filledLength + '-' * (length - filledLength)
    print('\r%s |%s| %s%% %s' % (prefix, bar, percent, suffix), end=printEnd)
    # Print New Line on Complete
    if iteration == total:
        print()


def weighted_quantile(values, quantiles, sample_weight=None,
                      values_sorted=False, old_style=False):
    """ Very close to numpy.percentile, but supports weights.
    NOTE: quantiles should be in [0, 1]!
    :param values: numpy.array with data
    :param quantiles: array-like with many quantiles needed
    :param sample_weight: array-like of the same length as `array`
    :param values_sorted: bool, if True, then will avoid sorting of
        initial array
    :param old_style: if True, will correct output to be consistent
        with numpy.percentile.
    :return: numpy.array with computed quantiles.
    """
    values = np.array(values)
    quantiles = np.array(quantiles)
    if sample_weight is None:
        sample_weight = np.ones(len(values))
    sample_weight = np.array(sample_weight)
    assert np.all(quantiles >= 0) and np.all(quantiles <= 1), \
        'quantiles should be in [0, 1]'

    if not values_sorted:
        sorter = np.argsort(values)
        values = values[sorter]
        sample_weight = sample_weight[sorter]

    weighted_quantiles = np.cumsum(sample_weight) - 0.5 * sample_weight
    if old_style:
        # To be convenient with numpy.percentile
        weighted_quantiles -= weighted_quantiles[0]
        weighted_quantiles /= weighted_quantiles[-1]
    else:
        weighted_quantiles /= np.sum(sample_weight)
    return np.interp(quantiles, weighted_quantiles, values)


class posterior(rv_continuous):
    def _pdf(self, x, samples, weights):
        pdf, edges = np.histogram(samples, weights=weights,
                                  bins=30, density=True)

        # func=scipy.interpolate.interp1d([(edges[i]+edges[i+1])/2 for i in range(len(edges)-1)],
        #                                pdf/np.max(pdf),fill_value=0,bounds_error=False)
        func = scipy.interpolate.interp1d(
            samples, weights/np.sum(weights), fill_value=0, bounds_error=False)
        return(func(x))

    def _logpdf(self, samples, weights):
        pdf, edges = np.histogram(samples, weights=weights,
                                  bins=30, density=True)

        # func=scipy.interpolate.interp1d([(edges[i]+edges[i+1])/2 for i in range(len(edges)-1)],
        #                                pdf/np.max(pdf),fill_value=0,bounds_error=False)
        func = scipy.interpolate.interp1d(samples, np.log(weights/np.sum(weights)),
                                          fill_value=-np.inf, bounds_error=False)
        return(func)

    def _argcheck(self, *args):
        return True


class NDposterior(rv_continuous):
    def _pdf(self, x, samples, weights):
        # pdf,edges=np.histogram(samples,weights=weights,
        #                       bins=30,density=True)

        # func=scipy.interpolate.interp1d([(edges[i]+edges[i+1])/2 for i in range(len(edges)-1)],
        #                                pdf/np.max(pdf),fill_value=0,bounds_error=False)
        func = scipy.interpolate.interp1d(
            samples, weights/np.sum(weights), fill_value=0, bounds_error=False)
        return(func(x))

    def _logpdf(self, samples, weights):
        # pdf,edges=np.histogram(samples,weights=weights,
        #                       bins=30,density=True)

        # func=scipy.interpolate.interp1d([(edges[i]+edges[i+1])/2 for i in range(len(edges)-1)],
        #                                pdf/np.max(pdf),fill_value=0,bounds_error=False)
        # func=scipy.interpolate.interp1d(samples,np.log(weights/np.sum(weights)),
        #    fill_value=-np.inf,bounds_error=False)
        if len(samples[0])>1:
            func = scipy.interpolate.LinearNDInterpolator(samples,
                                                      np.log(weights/np.sum(weights)), fill_value=-np.inf)
        else:
            samples = np.array([x[0] for x in samples])
            func = scipy.interpolate.interp1d(samples,
                                                      np.log(weights/np.sum(weights)), fill_value=-np.inf,bounds_error=False)
        return(func)

    def _argcheck(self, *args):
        return True


def guess_magnifications(curves, referenceImage):
    """Guess t0 and amplitude of the model based on the data.

    Assumes the data has been standardized."""
    ref = None
    mags = dict([])
    all_ims = [referenceImage]
    for k in [x for x in curves.images.keys() if x != referenceImage]:
        all_ims.append(k)
    for k in all_ims:
        if not curves.images[k].fits:
            bestRatio = -np.inf
            bestBand = None
            for b in np.unique(curves.images[k].table['band']):
                ratio = np.abs(np.max(curves.images[k].table['flux'][curves.images[k].table['band'] == b])/np.min(
                    curves.images[k].table['flux'][curves.images[k].table['band'] == b]))
                if ratio > bestRatio:
                    bestRatio = ratio
                    bestBand = b
            maxTime, maxValue = _findMax(curves.images[k].table['time'][curves.images[k].table['band'] == b],
                                         curves.images[k].table['flux'][curves.images[k].table['band'] == b])

            if k == referenceImage:
                ref = copy(maxValue)
            mags[k] = maxValue/ref

        else:
            if 'x0' in curves.images[k].fits.res.vparam_names:
                amplitude = 'x0'
            else:
                amplitude = 'amplitude'
            mag = curves.images[k].fits.model.get(amplitude)
            if k == referenceImage:
                ref = copy(mag)
            mags[k] = mag/ref
    return mags


def guess_time_delays(curves, referenceImage):
    # tds=colorFit(curves)
    # if tds:
    #    return tds
    ref = None
    tds = dict([])
    all_ims = [referenceImage]
    for k in [x for x in curves.images.keys() if x != referenceImage]:
        all_ims.append(k)
    for k in all_ims:
        if not curves.images[k].fits:
            maxValue, maxFlux = _findMax(
                curves.images[k].table['time'], curves.images[k].table['flux'])
            if k == referenceImage:
                ref = copy(maxValue)
            tds[k] = maxValue-ref
        else:
            t0 = curves.images[k].fits.model.get('t0')
            if k == referenceImage:
                ref = copy(t0)
            tds[k] = t0-ref
    return tds


def _findMax(time, curve):
    # TODO check edge cases
    t0 = np.where(curve == np.max(curve))[0][0]
    if t0 == 0:
        # return(time[0])
        return (time[0], curve[0])

    elif t0 == len(time)-1:

        return(time[-1], curve[-1])
    #   return (None,None)

    else:
        fit = splrep(time[t0-1:t0+2], curve[t0-1:t0+2], k=2)

    interptime = np.linspace(time[t0-1], time[t0+1], 100)
    flux = splev(interptime, fit)
    return(interptime[flux == np.max(flux)], np.max(flux))


def _findMin(time, curve):
    # TODO check edge cases
    t0 = np.where(curve == np.min(curve))[0][0]

    if t0 == 0:
        # return(time[0])
        return (None, None)

    elif t0 == len(time)-1:
        # return(time[-1])
        return (None, None)

    else:
        fit = splrep(time[t0-1:t0+2], curve[t0-1:t0+2], k=2)

    interptime = np.linspace(time[t0-1], time[t0+1], 100)
    flux = splev(interptime, fit)
    return(interptime[flux == np.min(flux)], np.max(flux))


def flux_to_mag(table, bandDict, zpsys='AB'):
    """
    Accepts an astropy table of flux data and does the conversion to mags (flux in ergs/s/cm^2/AA)

    :param table: Table containing flux, flux error error, and band columns
    :type table: astropy.Table
    :param bandDict: translates band to sncosmo.Bandpass object (i.e. 'U'-->bessellux)
    :type bandDict: dict
    :param zpsys: magnitude system
    :type zpsys: str,optional
    :returns: astropy.Table object with mag and mag error added
    """
    table = table[table['flux'] > 0]
    ms = sncosmo.get_magsystem(zpsys)
    table[_get_default_prop_name('mag')] = np.asarray(map(lambda x, y: ms.band_flux_to_mag(x, y), table[_get_default_prop_name('flux')],  # /sncosmo.constants.HC_ERG_AA
                                                          np.array([bandDict[z] for z in table[_get_default_prop_name('band')]])))
    table[_get_default_prop_name('magerr')] = np.asarray(map(lambda x, y: 2.5 * np.log10(np.e) * y / x, table[_get_default_prop_name('flux')],
                                                             table[_get_default_prop_name('fluxerr')]))
    table = table[np.abs(table['mag']/table['magerr']) > 5]
    return(table)


def _cast_str(s):
    try:
        return int(s)
    except:
        try:
            return float(s)
        except:
            return s.strip()


def get_default_prop_name(prop):
    for key, value in _props.items():
        if {prop.lower()} & value:
            return key
    return prop


def _isfloat(value):
    try:
        float(value)
        return True
    except:
        return False


def anyOpen(filename, mode='r', buff=1024*1024, external=PARALLEL):
    if 'r' in mode and 'w' in mode:
        return None
    if filename.startswith('!'):
        import subprocess
        if 'r' in mode:
            return subprocess.Popen(filename[1:], shell=True, bufsize=buff,
                                    stdout=subprocess.PIPE).stdout
        elif 'w' in mode:
            return subprocess.Popen(filename[1:], shell=True, bufsize=buff,
                                    stdin=subprocess.PIPE).stdin
    elif filename.endswith('.bz2'):
        if external == NORMAL:
            import bz2
            return bz2.BZ2File(filename, mode, buff)
        elif external == PROCESS:
            if not which('bzip2'):
                return anyOpen(filename, mode, buff, NORMAL)
            if 'r' in mode:
                return anyOpen('!bzip2 -dc ' + filename, mode, buff)
            elif 'w' in mode:
                return anyOpen('!bzip2 >' + filename, mode, buff)
        elif external == PARALLEL:
            if not which('pbzip2'):
                return anyOpen(filename, mode, buff, PROCESS)
            if 'r' in mode:
                return anyOpen('!pbzip2 -dc ' + filename, mode, buff)
            elif 'w' in mode:
                return anyOpen('!pbzip2 >' + filename, mode, buff)
    elif filename.endswith('.gz'):
        if external == NORMAL:
            import gzip
            return gzip.GzipFile(filename, mode, buff)
        elif external == PROCESS:
            if not which('gzip'):
                return anyOpen(filename, mode, buff, NORMAL)
            if 'r' in mode:
                return anyOpen('!gzip -dc ' + filename, mode, buff)
            elif 'w' in mode:
                return anyOpen('!gzip >' + filename, mode, buff)
        elif external == PARALLEL:
            if not which('pigz'):
                return anyOpen(filename, mode, buff, PROCESS)
            if 'r' in mode:
                return anyOpen('!pigz -dc ' + filename, mode, buff)
            elif 'w' in mode:
                return anyOpen('!pigz >' + filename, mode, buff)
    elif filename.endswith('.xz'):
        if which('xz'):
            if 'r' in mode:
                return anyOpen('!xz -dc ' + filename, mode, buff)
            elif 'w' in mode:
                return anyOpen('!xz >' + filename, mode, buff)
    else:
        return open(filename, mode, buff)
    return None


def which(program):
    import os

    def is_exe(fpath):
        return os.path.isfile(fpath) and os.access(fpath, os.X_OK)

    fpath, fname = os.path.split(program)
    if fpath:
        if is_exe(program):
            return program
    else:
        for path in os.environ["PATH"].split(os.pathsep):
            path = path.strip('"')
            exe_file = os.path.join(path, program)
            if is_exe(exe_file):
                return exe_file

    return None
